{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr3AcDe66QJU7g7E4heWal",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denis-shema/aiclass/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai -q\n",
        "!pip install langchain-community -q\n",
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "id": "6yGxlYjsa4v1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1def0975-8285-4ba1-eaf7-06fd9297e29e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "class ZhipuAI_embeddings:\n",
        "    def __init__(self, model_name: str = 'embeddings-3'):\n",
        "        self.model_name = model_name\n",
        "        self.base_url = \"https://open.bigmodel.cn/api/paas/v4\"\n",
        "        self.embedding = self._init_model()\n",
        "    def _init_model(self) -> OpenAIEmbeddings:\n",
        "        return OpenAIEmbeddings(\n",
        "            model=self.model_name,\n",
        "            base_url=self.base_url,\n",
        "            api_key=userdata.get(\"API_key\")\n",
        "        )\n",
        "embeddings =ZhipuAI_embeddings().embedding"
      ],
      "metadata": {
        "id": "VZDo8QvVa4rQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "client  = ChatOpenAI(\n",
        "    base_url =\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "    api_key = userdata.get(\"API_key\"),\n",
        "    model = \"glm-4.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ibet52tja4nv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.invoke(\"hello\")"
      ],
      "metadata": {
        "id": "9LxWZBVDa4kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b38efc8-308a-4b53-a162-805ba0449022"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 8, 'total_tokens': 21, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'glm-4.5', 'system_fingerprint': None, 'id': '202509121419322cb3d3c02ecc440a', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--fd99c11f-dfb9-4adf-8ca5-ba2cc26b5232-0', usage_metadata={'input_tokens': 8, 'output_tokens': 13, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "def doc_parsing(file_path) -> list[Document]:\n",
        "    if file_path.endswith(\"pdf\"):\n",
        "        doc = PyPDFLoader(file_path=file_path)\n",
        "        doc = doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=30\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        print(f\" the number of chucks :{len(raw_chunks)}\")\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    elif file_path.endswith(\".txt\"):\n",
        "        doc = TextLoader(file_path=file_path)\n",
        "        doc =doc.load()\n",
        "        semantic_splitter = SemanticChunker(\n",
        "            embeddings=embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=95\n",
        "        )\n",
        "        full_text = doc[0].page_content if doc else \"\"\n",
        "        if not full_text:\n",
        "            return []\n",
        "        raw_chunks = semantic_splitter.split_text(full_text)\n",
        "        docs = [Document(page_content=chunk, metadata=doc[0].metadata) for chunk in raw_chunks]\n",
        "        return docs\n",
        "    else:\n",
        "       return []"
      ],
      "metadata": {
        "id": "BQevgmhca4UA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_template = \"\"\"\n",
        "Your name is AICLASS assistant a smart, yet sassy assistant working at takenolab, your have a better knowledge of takenolab operations,\n",
        "your task is to be supportive, provide proper guidance to students that are having troubles with the course content,\n",
        "you have to answer them direct and precise, if you dont have any advice to them dont generate any respose kindly\n",
        "tell them so.\n",
        "When you receive:\n",
        "• question: the student’s problem\n",
        "• context: optionally, the content of any uploaded documents\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "new_template = \"\"\"\n",
        "Your name is AICLASS assistant, a smart, yet sassy assistant working at takenolab. You have a deep knowledge of takenolab operations.\n",
        "Your task is to be supportive, provide proper guidance to students who are having trouble with the course content.\n",
        "You must answer them directly and precisely. If you don't have any advice for them, kindly tell them so and do not generate any other response.\n",
        "\n",
        "When you receive:\n",
        "• chat_history: the previous turns of the conversation (if any)\n",
        "• question: the student’s current problem\n",
        "• context: optionally, the content of any uploaded documents relevant to the current question\n",
        "\n",
        "If `context` is non-empty, you **must** use it to inform your answer. If you don’t have enough information\n",
        "from the question and context combined, tell the student you can’t help further.\n",
        "If `chat_history` is provided, use it to understand the full context of the current question.\n",
        "\n",
        "<chat_history>\n",
        "{chat_history}\n",
        "</chat_history>\n",
        "\n",
        "student problem:\n",
        "{question}\n",
        "\n",
        "uploaded document context (if any):\n",
        "{context}\n",
        "\n",
        "your smart advice or solution:\n",
        "\n",
        "\"\"\"\n",
        "SUB_QUERY_TEMPLATE = \"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate {num_queries} diverse search queries related to the user's question, which can be used to retrieve relevant documents.\n",
        "The queries should be concise and cover different aspects or angles of the original question.\n",
        "\n",
        "Original Question: {question}\n",
        "\n",
        "Generated Queries:\n",
        "-\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "npS5k9AQa4FJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Wwq8FlhKavdm",
        "outputId": "ab80b116-7891-4283-a200-68efccb03748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3ebc5a2138c60d4856.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3ebc5a2138c60d4856.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List, Optional, Tuple, Dict\n",
        "import gradio as gr\n",
        "from google.colab import userdata # Import userdata\n",
        "\n",
        "@tool\n",
        "def define_term(term: str) -> str:\n",
        "    \"\"\"Defines a given term.\"\"\" # Added docstring\n",
        "    return f\"{term} is a placeholder definition for demonstration purposes.\"\n",
        "\n",
        "@tool\n",
        "def summarize_notes(text: str) -> str:\n",
        "    \"\"\"Summarizes the provided text.\"\"\" # Added docstring\n",
        "    return \"This is a placeholder summary based on the input text.\"\n",
        "\n",
        "tools = [define_term, summarize_notes]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key=userdata.get(\"API_key\")).bind_tools(tools) # Added api_key\n",
        "\n",
        "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
        "\n",
        "new_template = \"\"\"You are a helpful assistant. Use the following context and chat history to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "SUB_QUERY_TEMPLATE = \"\"\"Break down the following question into {num_queries} sub-questions:\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "def generate_sub_queries(original_question: str, num_queries: int = 3) -> List[str]:\n",
        "    sub_query_prompt = PromptTemplate.from_template(template=SUB_QUERY_TEMPLATE)\n",
        "    sub_query_chain = sub_query_prompt | llm\n",
        "    response = sub_query_chain.invoke({\n",
        "        \"question\": original_question,\n",
        "        \"num_queries\": num_queries\n",
        "    })\n",
        "    queries = [q.strip() for q in response.content.split('-') if q.strip()]\n",
        "    return queries\n",
        "\n",
        "def call_assistant(question: str = \"\", context_docs: List[Optional[Document]] = None, chat_history: List[Tuple[str, str]] = None):\n",
        "    prompt_template = PromptTemplate.from_template(template=new_template)\n",
        "    chain = prompt_template | llm\n",
        "    history_str = \"\"\n",
        "    if chat_history:\n",
        "        for human_msg, ai_msg in chat_history:\n",
        "            history_str += f\"User: {human_msg}\\nAssistant: {ai_msg}\\n\"\n",
        "    context_text = \"\"\n",
        "    if context_docs:\n",
        "        context_text = \"\\n\\n\".join(d.page_content for d in context_docs)\n",
        "    response = chain.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context_text,\n",
        "        \"chat_history\": history_str\n",
        "    })\n",
        "    return response.content\n",
        "\n",
        "def retrieve_and_answer_with_history(question: str, chat_history: List[Dict]) -> str:\n",
        "    formatted_chat_history_for_llm = []\n",
        "    current_user_msg = None\n",
        "    for msg in chat_history:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            current_user_msg = msg[\"content\"]\n",
        "        elif msg[\"role\"] == \"assistant\" and current_user_msg:\n",
        "            formatted_chat_history_for_llm.append((current_user_msg, msg[\"content\"]))\n",
        "            current_user_msg = None\n",
        "\n",
        "    if len(vector_store.store.items()) > 0:\n",
        "        transformed_queries = generate_sub_queries(question, num_queries=3)\n",
        "        all_retrieved_docs = []\n",
        "        seen_doc_contents = set()\n",
        "        for query in transformed_queries:\n",
        "            retrieved_for_query = vector_store.similarity_search(query)\n",
        "            for doc in retrieved_for_query:\n",
        "                if doc.page_content not in seen_doc_contents:\n",
        "                    all_retrieved_docs.append(doc)\n",
        "                    seen_doc_contents.add(doc.page_content)\n",
        "        return call_assistant(question, context_docs=all_retrieved_docs, chat_history=formatted_chat_history_for_llm)\n",
        "    return call_assistant(question, chat_history=formatted_chat_history_for_llm)\n",
        "\n",
        "def doc_loader(file_path):\n",
        "    docs = doc_parsing(file_path)\n",
        "    if not docs:\n",
        "        return \"No content found or processed in the document.\"\n",
        "    _ = vector_store.add_documents(documents=docs)\n",
        "    return docs[0].page_content[:200] + \"...\"\n",
        "\n",
        "def interface():\n",
        "    iface = gr.ChatInterface(\n",
        "        fn=retrieve_and_answer_with_history,\n",
        "        chatbot=gr.Chatbot(height=200, type='messages', label=\"Assistant\"),\n",
        "        textbox=gr.Textbox(lines=2, submit_btn=True),\n",
        "        title=\"Takenolab AIClass Assistant (Conversational RAG)\",\n",
        "        type='messages',\n",
        "        description=\"Ask a question about your course content and get smart advice, supporting multi-turn conversations.\",\n",
        "    )\n",
        "\n",
        "    docs_interface = gr.Interface(\n",
        "        fn=doc_loader,\n",
        "        inputs=gr.File(label=\"Choose a file to upload\", type='filepath', file_count='single'),\n",
        "        outputs=gr.TextArea(),\n",
        "        description=\"Upload a document to run retrieval-augmented generation.\"\n",
        "    )\n",
        "\n",
        "    table = gr.TabbedInterface(\n",
        "        [iface, docs_interface],\n",
        "        tab_names=['Chat', \"Upload File for RAG\"],\n",
        "        title=\"LLM, RAG AND PROMPTS, Text Generation\"\n",
        "    )\n",
        "    table.launch(debug=True, server_port=3000)\n",
        "\n",
        "interface()"
      ]
    }
  ]
}